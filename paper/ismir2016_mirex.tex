% -----------------------------------------------
% Template for ISMIR Papers
% 2016 version, based on previous ISMIR templates

% Requirements :
% * 6+1 page length maximum
% * 2MB maximum file size
% * Copyright note must appear in the bottom left corner of first page
% (see conference website for additional details)
% -----------------------------------------------

\documentclass{article}
\usepackage{ismir,amsmath,cite}
\usepackage{graphicx}
\usepackage{color}

% Title.
% ------
\title{A plan for sustainable MIR evaluation}

% Note: Please do NOT use \thanks or a \footnote in any of the author markup

% Single address
% To use with only one author or several with the same address
% ---------------
%\oneauthor
% {Names should be omitted for double-blind reviewing}
% {Affiliations should be omitted for double-blind reviewing}

% Two addresses
% --------------
%\twoauthors
%  {First author} {School \\ Department}
%  {Second author} {Company \\ Address}

%% To make customize author list in Creative Common license, uncomment and customize the next line
%  \def\authorname{First Author, Second Author} 


% Three addresses
% --------------
\threeauthors
  {First Author} {Affiliation1 \\ {\tt author1@ismir.edu}}
  {Second Author} {\bf Retain these fake authors in\\\bf submission to preserve the formatting}
  {Third Author} {Affiliation3 \\ {\tt author3@ismir.edu}}

%% To make customize author list in Creative Common license, uncomment and customize the next line
%  \def\authorname{First Author, Second Author, Third Author} 

% Four or more addresses
% OR alternative format for large number of co-authors
% ------------
%\multauthor
%{First author$^1$ \hspace{1cm} Second author$^1$ \hspace{1cm} Third author$^2$} { \bfseries{Fourth author$^3$ \hspace{1cm} Fifth author$^2$ \hspace{1cm} Sixth author$^1$}\\
%  $^1$ Department of Computer Science, University , Country\\
%$^2$ International Laboratories, City, Country\\
%$^3$  Company, Address\\
%{\tt\small CorrespondenceAuthor@ismir.edu, PossibleOtherAuthor@ismir.edu}
%}
%\def\authorname{First author, Second author, Third author, Fourth author, Fifth author, Sixth author}


\sloppy % please retain sloppy command for improved formatting

\begin{document}

%
\maketitle
%
\begin{abstract}
% MIREX is great!
The Music Information Retrieval Evaluation eXchange (MIREX) is a valuable community service, having established standard datasets, metrics, baselines, methodologies, and infrastructure for comparing MIR methods. 
% ...but we can't keep this up. sad!
While MIREX has managed to successfully maintain operations for over a decade, its long-term sustainability is at risk without considerable ongoing financial support. 
% Main problem: the expenditure of effort is intrinsically unsustainable.
The imposed constraint that input data cannot be made freely available to participants
necessitates that all algorithms run on centralized computational resources, which are 
administered by a limited number of people.
This incurs an approximately linear cost with the number of submissions, exacting
significant tolls on both human and financial resources, such that the current paradigm
becomes \emph{less} tenable as participation increases.
% what's worse, data depletion is unavoidable.
%Meanwhile, successive benchmarking iterations unavoidably deplete the value of annotated data for evaluation purposes, and is a resource that requires constant replenishing. % replacement?
% Main idea: Use one problem to solve the other.
To alleviate the recurring costs of future evaluation campaigns, we propose a distributed,
community-centric paradigm for system evaluation, built upon the principles of openness,
transparency, reproducibility, and incremental evaluation.
%where users contribute annotations over publicly available audio content in order to participate. 
% Benefits: sustainability, scalability,  
%In this document, we outline the goals, benefits, and limitations of such an approach. 
%describe an alternative framework 
We argue that this proposal has the potential to reduce operating costs to sustainable
levels.
Moreover, the proposed paradigm would improve scalability, and eventually result in the
release of large, open datasets for improving both MIR techniques and evaluation methods.

\end{abstract}
%

\section{Introduction}

MIREX has run for several years, and serves a valuable purpose for the community.  It also has a few drawbacks and problems of sustainability.  This document will lay out the goals, benefits, and limitations of MIREX.  We’ll then propose an alternative system which should be more sustainable.


\section{MIREX}

MIREX provides a platform for common evaluation of music annotation algorithms.  The general strategy operates as follows:
Define some task of interest.  Say, chord recognition.
Collect human-annotated data for the task.
Release a small set of the data for development purposes.  Retain the rest as private data for evaluation.
Invite authors to submit code, which is executed on a private server.  Report the results.
Repeat step 4.  Intermittently repeat step 2.

(Note: this ignores some tasks, such as audio music similarity, which require human annotators to rate the outputs of each algorithm.  We’ll come back to that later.)

MIREX retains private data sets to stave off the effects of over-fitting to the evaluation set. (If participants don’t see the data, it’s harder to overfit.  In theory.)

It has been argued that retaining private data is more cost-effective, since Step 2. above is typically the most expensive.  If data sets are not released, they can be reused year upon year.  (There is also the issue of dealing with obtaining distribution licenses for musical content, which we’ll return to below.)  However, systems will still eventually over-fit by trial and error, so there is no avoiding the data collection issue in the long run.  Moreover, step 4 requires a considerable amount of volunteer effort to maintain, through the so-called “task captains”, as well as various administration and maintenance issues.  This results in a system that is ultimately unsustainable, as the costs are proportional to participation (which we should hope would grow), rather than data (which would be constant with respect to the number of participants).


\section{Open music evaluation}

An alternative to the MIREX framework is Kaggle.  In Kaggle, all input data is made publicly available, and participants submit only the outputs of their systems.  Here, the major cost bottleneck is distributed to the participants, rather than centralized, thus reducing the maintenance and administration costs.  However, there is a greater risk of overfit (and the potential for cheating), since the input data is now public.

Let’s temporarily assume that participants are honest, and do not cheat.  There is still the matter of over-fitting the data.  Within a single run, this problem can be partially dealt with in a couple of ways:
use large test collections, so that it’s impossible (or impractical) to manually fit to the test
only evaluate on a random subset of the test data
After scores are released from an evaluation campaign, there is still the possibility of long-range over-fit through the feedback loop of score reporting.  To counteract this, different test collections should be introduced each year.  At first, this seems to get us nowhere: we still need to acquire annotated data, which is presumably expensive.  I argue that collecting annotations is a better use of resources (both time and funding) than maintaining server execution, as publicly available data benefits many more people than the mirex participants for any given year.

There are some remaining issues to deal with here:
annotation acquisition
content selection and sharing
participation / honesty

\subsection{Data acquisition}

Ideally, the evaluation data should grow, year upon year, with old data being released to the public either annually or biannually.  To facilitate this, I propose a data credit system.

To participate in an evaluation, each participant must “pay” some nominal amount of data credits.  In the first year, all participants start with a balance of 0, so by participating, they are now in “data debt”.  Data credits can be earned by providing annotations to be used in the following year’s campaign.  A participant is eligible for evaluation in year X when their data credit reaches a non-negative value.

Data credits can be retained across years, and transferred across tasks.


The credits earned can be task-dependent.  Chord annotations might be worth more than beat timings or tempo estimations, for instance.

Data credits may also be earned at a discounted rate by auditing or validating previously provided annotations?  We’ll need a system in place to authenticate annotators and force independence.

Potentially, we may institute a cap-and-trade system to exchange data credits for money which can be used to pay for expert annotations.  Not sure about the legal ramifications of implementing this, or how to organize efforts.  Aside from cash exchange, there might be a benefit to having a task exchange: eg, “i’ll trade three beat annotations for one chord annotation”.


\subsection{Content selection}

There are several competing interests in selecting content for inclusion in evaluation:
availability and distributability of content
quality control
utility of having the annotation
coverage and representation bias

The first point can be addressed by using only creative commons licensed music.  Luckily, a wealth of this data is available through the http://freemusicarchive.org/.

The second point is a difficult one, though FMA does provide some curation.  It might be interesting to partner with them in helping to guide the selection

The third point raises an interesting technical challenge.  Which tracks is it worth annotating? Several recent studies have investigated the use of algorithm disagreement for detecting “hard” examples, be it for beat tracking, structural analysis, or chord recognition.

In the proposed framework, we’re in a fortunate position of having exactly this type of functionality available to us: participants are submitting system outputs for evaluation!  It need not be the case that all of the data is being used for evaluation: in fact, Kaggle shows that it’s helpful to cloud the space with data that are not included in testing.  If we’re careful about selecting these data, we can then compare the outputs across the participating systems to see which ones are “interesting”: if the systems generally agree, the example is likely to be easy; at the very least, annotating this track will not help distinguish between these systems.  If the systems disagree, then we should definitely solicit a strong annotation for the track.  This is essentially the point made by Urbano et al., 2012.

Each track marked as “interesting” should be annotated by at least two humans to determine whether there’s actual disagreement.  It can then be annotated in year 2, and enter the evaluation in year 3.  By having multiple reference annotations for the “interesting” tracks, we can compare algorithm agreement to annotator agreement, and distinguish “difficult” tracks from “ambiguous” tracks.

Finally, the fourth point above can be integrated into point 2 by soliciting recommendations from the community.

\section{Incremental evaluation}


\section{Discussion}

No free lunch theorem holds.

Participation and Honesty

We ultimately rely on participants to be honest in their methods and reporting.  What’s to prevent a participant from manually annotating all potential test tracks?

I expect that this situation is unlikely.  (However, if people do cheat, we at least get the annotation data, so we still win.)

We may also encourage (require?) participants to release source code to reproduce their methods.  This is in keeping with the spirit of open science and reproducible methods.  I doubt there will be any way to enforce this systematically, but it could be used to audit systems post-hoc.

There’s also the matter of selecting which tracks to use in each task.  It may make sense to release a single, large dataset, and require participants to submit annotations for all tracks and for all tasks in which they wish to compete.  If we start getting into the business of slicing down to subsets per task, it will become much easier to determine which tracks are used for which task, and we don’t want that.  This all, of course, incurs a greater computational overhead on the participants, but this is A) worth it, from a data collection perspective, and B) how these systems should be used in practice anyway.

We may also consider a two-tiered system, in which closed systems are ranked separately from open systems, which provide voluntary source code.  This may help ease the burden of going fully open source, and encourage submissions from industry.  I think this is a bad idea though.  If it isn’t open, it isn’t science.

Coverage, or “What tasks are left out?”

The system outlined above can apply to any task for which outputs are evaluated by comparing against a reference.  This includes basically everything except for AMS/SMS, where outputs are scored directly.  I argue that these tasks, while interesting, are different enough to fall beyond the scope of the current framework, and may best be left to a separate evaluation campaign.

Alternately, we can treat these left-out tasks as a way to pivot traditional mirex to focus on things that require direct human evaluation, eg, annual grand challenge tasks.  This way, Downie’s crew gets to keep their current job, and everyone else is happy.

Content Validity

Music is a unique information signal in that its perception is impacted by cultural significance and how this influences expectation and novelty in a listener. Musical content from the Free Music Archive, or other creative commons sources, are likely to be outside of the gamut of commonly studied music, e.g., popular Western music, and differences in compositional style, instrumentation, or production, may lead to difficulties in validation and generalization. While this is unlikely to affect tasks such as onset detection or instrument identification, higher level tasks that place greater demand on cognitive aspects of music perception, such as chord estimation or structural analysis, are more sensitive to biased collections. 

Hosting

We’ll need some infrastructure to actually run the campaign in the new setup.  I think CodaLab might have solved a good chunk of this problem for us.

To-do

To get this off the ground, we’ll need a few things:
Evaluation services (already mostly provided by mir_eval)
An initial seed pool of data (can be inherited from the existing mirex)
A pool of candidate data for evaluation
Infrastructure for managing the exchange
some math / stats to determine how much data we need for each task, each year
some way to work out the data credit rates for different tasks

Misc Refs
Separate “labels” for systems depending on what they do (eg. close/open, full/subset of dataset): http://trec.nist.gov/pubs/trec22/papers/WEB.OVERVIEW.pdf 
Previous proposals (section 8): http://julian-urbano.info/files/publications/051-evaluation-music-information-retrieval.pdf


% For bibtex users:
\bibliography{ISMIRtemplate}

\end{document}
