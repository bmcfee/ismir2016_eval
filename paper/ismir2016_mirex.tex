% -----------------------------------------------
% Template for ISMIR Papers
% 2016 version, based on previous ISMIR templates

% Requirements :
% * 6+1 page length maximum
% * 2MB maximum file size
% * Copyright note must appear in the bottom left corner of first page
% (see conference website for additional details)
% -----------------------------------------------

\documentclass{article}
\usepackage{ismir,amsmath,cite}
\usepackage{graphicx}
\usepackage{color}

% Title.
% ------
\title{A Plan for Sustainable MIR Evaluation}

% Note: Please do NOT use \thanks or a \footnote in any of the author markup

% Single address
% To use with only one author or several with the same address
% ---------------
%\oneauthor
% {Names should be omitted for double-blind reviewing}
% {Affiliations should be omitted for double-blind reviewing}

% Two addresses
% --------------
%\twoauthors
%  {First author} {School \\ Department}
%  {Second author} {Company \\ Address}

%% To make customize author list in Creative Common license, uncomment and customize the next line
%  \def\authorname{First Author, Second Author}


% Three addresses
% --------------
\threeauthors
  {First Author} {Affiliation1 \\ {\tt author1@ismir.edu}}
  {Second Author} {\bf Retain these fake authors in\\\bf submission to preserve the formatting}
  {Third Author} {Affiliation3 \\ {\tt author3@ismir.edu}}

%% To make customize author list in Creative Common license, uncomment and customize the next line
%  \def\authorname{First Author, Second Author, Third Author}

% Four or more addresses
% OR alternative format for large number of co-authors
% ------------
%\multauthor
%{First author$^1$ \hspace{1cm} Second author$^1$ \hspace{1cm} Third author$^2$} { \bfseries{Fourth author$^3$ \hspace{1cm} Fifth author$^2$ \hspace{1cm} Sixth author$^1$}\\
%  $^1$ Department of Computer Science, University , Country\\
%$^2$ International Laboratories, City, Country\\
%$^3$  Company, Address\\
%{\tt\small CorrespondenceAuthor@ismir.edu, PossibleOtherAuthor@ismir.edu}
%}
%\def\authorname{First author, Second author, Third author, Fourth author, Fifth author, Sixth author}


\sloppy % please retain sloppy command for improved formatting

\begin{document}

%
\maketitle
%
\begin{abstract}
% MIREX is great!
The Music Information Retrieval Evaluation eXchange (MIREX) is a valuable community service, having established standard datasets, metrics, baselines, methodologies, and infrastructure for comparing MIR methods.
% ...but we can't keep this up. sad!
While MIREX has managed to successfully maintain operations for over a decade, its long-term sustainability is at risk without considerable ongoing financial support.
% Main problem: the expenditure of effort is intrinsically unsustainable.
The imposed constraint that input data cannot be made freely available to participants
necessitates that all algorithms run on centralized computational resources, which are
administered by a limited number of people.
This incurs an approximately linear cost with the number of submissions, exacting
significant tolls on both human and financial resources, such that the current paradigm
becomes \emph{less} tenable as participation increases.
% what's worse, data depletion is unavoidable.
%Meanwhile, successive benchmarking iterations unavoidably deplete the value of annotated data for evaluation purposes, and is a resource that requires constant replenishing. % replacement?
% Main idea: Use one problem to solve the other.
To alleviate the recurring costs of future evaluation campaigns, we propose a distributed,
community-centric paradigm for system evaluation, built upon the principles of openness,
transparency, reproducibility, and incremental evaluation.
%where users contribute annotations over publicly available audio content in order to participate.
% Benefits: sustainability, scalability,
%In this document, we outline the goals, benefits, and limitations of such an approach.
%describe an alternative framework
We argue that this proposal has the potential to reduce operating costs to sustainable
levels.
Moreover, the proposed paradigm would improve scalability, and eventually result in the
release of large, open datasets for improving both MIR techniques and evaluation methods.

\end{abstract}


\section{Introduction}
% TODO(ejhumphrey|bmcfee)

MIREX has run for several years, and serves a valuable purpose for the community.
It also has a few drawbacks and problems of sustainability.
This document will lay out the goals, benefits, and limitations of MIREX.
We'll then propose an alternative system which should be more sustainable.

% These are rough guesses. @bmcfee halp
The goals of this proposal are threefold:
one, to eventually achieve consensus in the community about how to best proceed with evaluation;
two, establish a feasible plan that is maintainable with minimal resources;
and three, begin to make inroads toward adoption.

\section{MIREX}

% History
The Music Information Retrieval eXcahnge is a framework for the community driven evaluation of MIR algorithms \cite{2008DownieMIREX, 2010DownieMIREX}.
The annual tradition of MIREX was established early in the lifetime of ISMIR, drawing inspiration from TREC in text IR \cite{TRECPaperQuestionmark}.
Thanks in large part to the vision of MIR pioneers, the first official iteration took place at ISMIR 2005 after much preliminary work, including a trial run the year prior called the Audio Description Contest (ADC).
The practicalities of MIREX are hosted by the IMIRSEL group at UIUC, and the organization has successfully earned multiple grants to jumpstart the evaluation effort at ISMIR.

% How does this work in practice
At a high level, the typical MIREX paradigm operates as follows:

\begin{enumerate}
\item Define some task of interest, such as automatic chord estimation, and arrive at a problem formulation and metrics with fellow researchers in the community.
\item Build a corpus of data, with human-annotated ``ground-truth'' annotations for said task.
\item Release a subset of the data for development purposes; retain the rest as private data for evaluation.
\item Invite participants to submit algorithms, which are executed on a private server. Report the results.
\item Repeat step 4. Intermittently repeat step 2, as needed.
\end{enumerate}

% Algorithm-to-data goes astray.
Note that, as a matter of correctness, some tasks, such as audio music similarity, rely on post-hoc human assessment, by design or necessity.
More importantly, however, MIREX differs from TREC-style evaluation in one critical way, named in (4) above:
MIREX operates in an ``algorithm-to-data'' model, where facilitators oversee the application of code submissions to privately held data, rather than participants submitting predictions over a freely available dataset.
The rationale for this decision is understandable.
In contrast to other machine perceptions domains, such as natural language processing, speech recognition, or computer vision, intellectual property and copyright law imposes stiff penalties on the illegal distribution of recorded music.
Due to a history of litigation from the Recording Industry Artists of America (RIAA), there is a pervasive sense of fear in the MIR community that sharing audio data would almost certainly result in crippling lawsuits \cite{2008DownieMIREX}.

% Deficiencies
% Operating costs are significant
Experience with MIREX over the last decade has more than demonstrated that the decision to bring algorithms to data comes with fundamental limitations.
First, as a matter of practicality, doing so incurs remarkably steep costs the community cannot hope to support indefinitely.
Running hundreds of research algorithms demands state of the art technology, and adequate compute cycles must either rented or purchased outright to achieve this.
More often than not, these algorithms are research prototypes, seldom optimized for efficiency.
While task-dependent runtime limits are placed on algorithm execution (between 6 and 72 hours), MIREX is responsible for months of compute time annually.
Note that these relatively low ceilings indicate how small the datasets used in evaluation are, and more data --a universally accepted ``good thing''-- would require an increase in execution time.
The financial burden of computation amounts to a rounding error, however, when compared with the human effort involved.
As a point of reference, MIREX 2007 required ``nearly 1000 person-hours'' to supervise the execution of 122 algorithms from 40 teams to completion \cite{2008DownieMIREX};
in the years since, average participation has climbed slightly ($\mu\approx145$, $\sigma \approx 14$).
Extrapolating, the last decade has likely consumed on the order of \emph{10,000} person-hours just bringing algorithms to data.
Not only is this rate unsustainable, but the combined operating costs only \emph{increase} the more successful the endeavor becomes.
Said differently, the worst thing that could happen to MIREX in its current form is growth.


\begin{align}
\label{eq:systematic_eval}
Y_i = \mathcal{A}(X_i)\\
Z_i = \mathcal{F}(X_i)\\
M_i = \mathcal{C}(Y_i, Z_i)
\end{align}

% Hard to do science
% ------------------
Operating costs aside, MIREX has afforded some valuable insight into the research of MIR systems\cite{2008DownieMIREX}.
Unfortunately, many scientific endeavors are largely impeded or, at worst, wholly obfuscated in the current paradigm.
To illustrate this reality, consider the standard approach to benchmarking MIR systems,
given in Eq \ref{eq:systematic_eval}.
% diagrammed in Figure \ref{fig:system_eval}.
An input, $X_i$, is observed by an annotator, $\mathcal{A}$, producing a ``gold-standard'' output, $Y_i$.
Similarly, a proposed computational model, $\mathcal{F}$, operates on the same input, producing a prediction, $Z_i$.
One or more comparison functions, $\mathcal{C}^R$, are applied on these two representations, yielding a number of performance measures, $M_i^R$.
This process is repeated over a collection of input-output pairs, $\mathcal{D}, |\mathcal{D}|=N$, and the sample-wise measures are aggregated into summary statistics, $S^R$, the reliability of which generally increases with $N$.
% Transparency and oversight are necessary, this stuff is hard and warrants independent corroboration.
A lack of transparency renders participants blind in a number of detrimental ways.
There is no public access to $X$, making it impossible to diagnose why an algorithm produced $Z$, or determine what should be done to make the system better.
Furthermore, $Y$ is meaningless without $X$, and must be trivially taken on face value.
By the same token, while $N$ varies from task to task, there is no way to guage the distribution of the data, or identify any incidental biases.
Limited insight exists into $\mathcal{A}$, the role that subjectivity may play in a given annotation, or the instructions provided when the annotation was initially performed.
As such, the very problem formulation is hidden and subject to drift as it is reinterpreted annually.
Traditionally $C$ are implemented with proprietary technologies, i.e. MATLAB, by a handful of individuals with minimal oversight from the community, raising concerns of technical correctness.
% This is harsh conjecture. Definitely revise to something justifiable, but I've seem to hit a wall.
And finally, there is little visibility into the evaluation machinery or its use, and there is ample anecdotal evidence surrounding questionable scientific practices.

% Data Replenishing
Ironically, due in no small part to the high financial and human costs of the algorithm-to-data model, the MIR community, and thus MIREX, suffers from a lack of annotated data.
The current datasets are insufficient for benchmarking algorithms, and there is no viable plan in place to grow the size of these datasets over time.
By and large, MIREX has relied on the generosity of external resources to expand the size of its evaluation datasets.
In some cases, like automatic chord estiamtion, the testing set largely lives in the light of day, evidenced by one team who effectively submitted a fingerprinting system and achieved near perfect scores\footnote{sneaky McVicar...}.
As a result, the community has begun to develop systems that are over-fitting by trial and error, proving that we cannot avoid data collection in the long run.


% Some of the single biggest issues are caused by witholding data used for evaluation from participants.
% To be fair, the rationale for doing so is understandable.
% It has been argued that private data sets should help stave off the effects of over-fitting, which in turn slows the velocity at which ``good'' data becomes stale, and must be replaced.
% This is desirable, as (2) above is typically the most expensive stage in the cycle.
% The decision to maintain closed data sets is thus viewed as cost-effective, allowing the same data to be reused year upon year.
% Furthermore, this also helps circumvent the issue of dealing with obtaining distribution licenses for musical content, an oft raised concern in the field.
% However, experience has demonstrated that the disadvantages outweigh the benefits.
% For one, systems will still eventually over-fit by trial and error, so there is no avoiding data collection in the long run.
% Moreover, (4) requires a considerable amount of volunteer effort to maintain, through the so-called ``task captains''ù, as well as various administration and maintenance issues.
% Given the overwhelming diversity of tools and technologies available for use in scientific computing, this task borders on Sisyphisean.

% Deficiencies

% Closed input data means that participants submit not results, but \emph{algorithms}, resulting in a number of negative repercussions.
% Practically speaking, task captains are responsible for getting these myriad solutions up and running on different system configurations than the one on which it was developed.

% What's more, this challenge becomes more difficult as the number of submissions increases.
% Participants have no insight into the content used for benchmarking or perspective on why their algorithm performed the way it did.

% Requires volunteers


\section{Open Evaluation of MIR Systems}

In summary, MIREX suffers from a few terminal deficiencies.
One, the financial cost and burden on human effort is large enough that it requires significant external investment.
Two, the recurring need for freshly annotated data is in addition to normal operating costs.
And three, the lack of transparency into the data and metrics used limits the value of the exercise.

% Here we outline the concrete details of our proposal.
Thus, to address these deficiencies, our proposed plan has three key differentiators from the MIREX model:
\begin{itemize}
\item Distributed computation reduces operating costs to a participant-invariant constant.
\item Open-source content selection enables visibility into the data used for evaluation,
\item Incremental evaluation encourages that results are scientifically meaningful by keeping hold-out data fresh, while steadily increasing the size of datasets available for development and evaluation.
\end{itemize}

% After scores are released from an evaluation campaign, there is still the possibility of long-range over-fit through the feedback loop of score reporting, from which MIREX already suffers.
% To counteract this, different test collections should be introduced each year.
% At first, this seems to get us nowhere: we still need to acquire annotated data, which is presumably expensive.
% We argue that collecting annotations is a better use of resources (both time and funding) than maintaining server execution, as publicly available data benefits many more people than the MIREX participants for any given year.


\subsection{Distributed Computation}
% TODO(bmcfee)

An alternative to the MIREX framework is Kaggle.
In Kaggle, all input data is made publicly available, and participants submit only the outputs of their systems.
Here, the major cost bottleneck is distributed to the participants, rather than centralized, thus reducing the maintenance and administration costs.
However, there is a greater risk of overfit (and the potential for cheating), since the input data is now freely available.

Let's temporarily assume that participants are honest, and do not cheat.
There is still the matter of over-fitting the data.
Within a single run, this problem can be partially dealt with in a couple of ways:
\begin{itemize}
\item Use large test collections, so that it's impossible (or impractical) to manually fit to the test
\item Only evaluate on a random subset of the test data
\end{itemize}


\subsection{Open Source Content Selection}
% TODO(bmcfee)

There are several competing interests in selecting content for inclusion in evaluation:
\begin{itemize}
\item availability and distributability of content
\item quality control
\item utility of having the annotation
\item coverage and representation bias
\end{itemize}

The first point can be addressed by using only creative commons licensed music.
Luckily, a wealth of this data is available through the http://freemusicarchive.org/.
Is jamendo still a thing that can be used?
Do we even try to mention youtube?
Partnerships with YouTube, Spotify, SoundCloud?

The second point is a difficult one, though FMA does provide some curation.
It might be interesting to partner with them in helping to guide the selection.

The third point raises an interesting technical challenge.
Which tracks are worth annotating?
Several recent studies have investigated the use of algorithm disagreement for detecting ``hard''ù examples, be it for beat tracking \cite{}, structural analysis \cite{}, or chord recognition \cite{}.

In the proposed framework, we are in a fortunate position of having exactly this type of functionality available to us: participants are submitting system outputs for evaluation!
It need not be the case that all of the data is being used for evaluation: in fact, Kaggle shows that it is helpful to cloud the space with data that are not included in testing.
If we are careful about selecting these data, we can then compare the outputs across the participating systems to see which ones are ``interesting''ù: if the systems generally agree, the example is likely to be easy; at the very least, annotating this track will not help distinguish between these systems.
If the systems disagree, then we should definitely solicit a strong annotation for the track.
This is essentially the point made by Urbano et al. \cite{2012UrbanoXYZ}.

Each track marked as ``interesting''ù should be annotated by at least two humans to determine whether there is actual disagreement.
It can then be annotated in year 2, and enter the evaluation in year 3.
By having multiple reference annotations for the ``interesting''ù tracks, we can compare algorithm agreement to annotator agreement, and distinguish ``difficult''ù tracks from ``ambiguous''ù tracks.

Finally, the fourth point above can be integrated into point 2 by soliciting recommendations from the community.


\subsection{Incremental Evaluation}
% TODO(julian-urbano)


Additionally:

\begin{itemize}
\item Annotation Standards and their enforcement
\item User-facing tools for performing annotations
\item Backend infrastructure for hosting, serving, and collecting submitted annotations (common repository)
\item Would it be possible to leverage citizen science platforms, Mechanical Turk, etc
\end{itemize}


\subsection{Putting it All Together}

Annually, this plan could proceed as follows, as shown in Figure \ref{fig:cycle}:

First is the \emph{Content Selection} stage.
Here, we provide ``stale'' annotated data --that which shouldn't be used for evaluation any longer-- as training data.
Then, to curate a test set, we combine data previously unseen by the community for which we have annotations, with new, unlabeled content.
This unlabeled content serves to both obscure the datapoints in the test set that \emph{are} annotated, as well as solicit predictions from a large number of automatic algorithms.

Next is the \emph{Competition}, a time-boxed interval spanning a few weeks or months.
Content is made available publicly available is made at the start, such that the training set is provided with labels and the entire test set, without. Some number of submissions are accepted over this internal from registered participants.
A standard format for predictions is defined for each task, and participants can check their submissions against a evaluator to immediately an inkling of how they perform.
This public leaderboard gives insight on some percent of the data. e.g. Kaggle, and at the end of the competition, results are tabulated over the entire holdout set.

After the completion officially closes comes \emph{Incremental Evaluation}. Using the predictions submitted, identify content worth annotating.
It is at this point we can additionally revisit questionable annotations, and perform meta-evalatuation of our common methodology.

Having identified content worthy of investing human effort, the \emph{Annotation} stage proceeds until the next competition.
This is an important distinction, as it takes the most time consuming process --humans describing music-- and largely decouples it as an independent resources.
Thus, annotation spools constantly as a background process, and a snapshot

This virtuous cycle can then repeat indefinitely.
Some particularities are worth mentioning.
For example, it may be necessary to include humans in the evaluation loop at the close of the competition stage, as in user-facing challenges or music similarity ranking.


\section{Discussion}

No free lunch theorem holds.
What follows are a number of topics that have cropped up in the course of writing the paper to date.

\subsection{Human Logistics}

How do we get people to participate / contribute (all jump at once).

At the evaluation level, how do we prevent people from cheating, encourage integrity and the like.
Scale is a good answer for this, but difficult (impossible) to have out of the gate.

We ultimately rely on participants to be honest in their methods and reporting.  What's to prevent a participant from manually annotating all potential test tracks?

I expect that this situation is unlikely.  (However, if people do cheat, we at least get the annotation data, so we still win.)

We may also encourage (require?) participants to release source code to reproduce their methods.  This is in keeping with the spirit of open science and reproducible methods.  I doubt there will be any way to enforce this systematically, but it could be used to audit systems post-hoc.

There's also the matter of selecting which tracks to use in each task.
It may make sense to release a single, large dataset, and require participants to submit annotations for all tracks and for all tasks in which they wish to compete.
If we start getting into the business of slicing down to subsets per task, it will become much easier to determine which tracks are used for which task, and we don't want that.
This all, of course, incurs a greater computational overhead on the participants, but this is A) worth it, from a data collection perspective, and B) how these systems should be used in practice anyway.

We may also consider a two-tiered system, in which closed systems are ranked separately from open systems, which provide voluntary source code.
This may help ease the burden of going fully open source, and encourage submissions from industry.
I think this is a bad idea though.
If it isn't open, it isn't science.
(Just make it cost money then. Which would require the org to NPO-ize).

\subsection{Coverage, or ``What tasks are left out?''}

The system outlined above can apply to any task for which outputs are evaluated by comparing against a reference.
This includes basically everything except for AMS/SMS, where outputs are scored directly.
I argue that these tasks, while interesting, are different enough to fall beyond the scope of the current framework, and may best be left to a separate evaluation campaign.

Alternately, we can treat these left-out tasks as a way to pivot traditional MIREX to focus on things that require direct human evaluation, eg, annual grand challenge tasks.
This way, the MIREX crew gets to keep their current job, and everyone else is happy.

\subsection{Content Validity}

Music is a unique information signal in that its perception is impacted by cultural significance and how this influences expectation and novelty in a listener.
Musical content from the Free Music Archive, or other creative commons sources, are likely to be outside of the gamut of commonly studied music, e.g., popular Western music, and differences in compositional style, instrumentation, or production, may lead to difficulties in validation and generalization.
While this is unlikely to affect tasks such as onset detection or instrument identification, higher level tasks that place greater demand on cognitive aspects of music perception, such as chord estimation or structural analysis, are more sensitive to biased collections.

\subsection{Hosting}

We'll need some infrastructure to actually run the campaign in the new setup.  I think CodaLab might have solved a good chunk of this problem for us.

\subsection{Pay-to-Play}

Ideally, the evaluation data should grow, year upon year, with old data being released to the public either annually or biannually.
To facilitate this, we propose a data credit system.

To participate in an evaluation, each participant must ``pay''ù some nominal amount of data credits.
In the first year, all participants start with a balance of 0, so by participating, they are now in ``data debt''ù.
Data credits can be earned by providing annotations to be used in the following year's campaign.
A participant is eligible for evaluation in year $X$ when their data credit reaches a non-negative value.

Data credits can be retained across years, and transferred across tasks.

The credits earned can be task-dependent.  Chord annotations might be worth more than beat timings or tempo estimations, for instance.

Data credits may also be earned at a discounted rate by auditing or validating previously provided annotations?  We'll need a system in place to authenticate annotators and force independence.

%Potentially, we may institute a cap-and-trade system to exchange data credits for money which can be used to pay for expert annotations.  Not sure about the legal ramifications of implementing this, or how to organize efforts.  Aside from cash exchange, there might be a benefit to having a task exchange: eg, ``ill trade three beat annotations for one chord annotationù.

\subsection{Next Steps}

To get this off the ground, we'll need a few things:
Evaluation services (already mostly provided by mir\_eval)
An initial seed pool of data (can be inherited from the existing mirex)
A pool of candidate data for evaluation
Infrastructure for managing the exchange
some math / stats to determine how much data we need for each task, each year
some way to work out the data credit rates for different tasks

\section{Misc Refs}
Separate ``labelsù'' for systems depending on what they do (eg. close/open, full/subset of dataset): http://trec.nist.gov/pubs/trec22/papers/WEB.OVERVIEW.pdf
Previous proposals (section 8): http://julian-urbano.info/files/publications/051-evaluation-music-information-retrieval.pdf


% For bibtex users:
\bibliography{ISMIRtemplate}

\end{document}
